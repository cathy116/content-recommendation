{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Retrieval stage, we try to use the following methods/models to retrieve the relevant documents:\n",
    "1. Item-based Collaborative Filtering\n",
    "   - We only consider the `star` information that users give to the business in the `yelp_academic_dataset_review.json` file.\n",
    "   - We use `csr_matrix` from `scipy` to store the user-item matrix (for sparse matrix).\n",
    "   - We use `sp_matmul_topn` from `sparse_dot_topn` to calculate the cosine similarity to between two businesses. \n",
    "     - Compare to the `cosine_similarity` from `sklearn`, `sp_matmul_topn` is much faster as it only calculates the top-n similar items.\n",
    "     - Compare to the `approximate_nearest_neighbors` from `annoy`, `sp_matmul_topn` is much faster when finding building the index, but slower when querying. However, Item-based Collaborative Filtering \n",
    "2. User-based Collaborative Filtering\n",
    "3. Deep Structured Semantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_lists, prefix_path, chunk_size=10000):\n",
    "    df_dict = {}\n",
    "    prefix_path += \"sampled_\"\n",
    "    for file in file_lists:\n",
    "        try:\n",
    "            df_chunks = []\n",
    "            total_records = 0\n",
    "\n",
    "            for chunk in pd.read_json(prefix_path + file, lines=True, chunksize=chunk_size):\n",
    "                df_chunks.append(chunk)\n",
    "                total_records += chunk.shape[0]\n",
    "\n",
    "            df = pd.concat(df_chunks, ignore_index=True)\n",
    "            df_dict[file] = df\n",
    "            print(f\"Total records in {file}: {df.shape[0]}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../data/'\n",
    "transit_bucket = 'raw_datasets/'\n",
    "target_bucket = 'yelp/'\n",
    "prefix_path = folder_path + transit_bucket + target_bucket\n",
    "file_list = [\n",
    "    \"yelp_academic_dataset_business.json\",\n",
    "    \"yelp_academic_dataset_review.json\",\n",
    "    # \"yelp_academic_dataset_tip.json\",\n",
    "    # \"yelp_academic_dataset_checkin.json\",\n",
    "    # \"yelp_academic_dataset_user.json\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in yelp_academic_dataset_business.json: 78059.\n",
      "Total records in yelp_academic_dataset_review.json: 980418.\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset(file_list, prefix_path)\n",
    "df_business = df[\"yelp_academic_dataset_business.json\"]\n",
    "df_review = df[\"yelp_academic_dataset_review.json\"]\n",
    "\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sparse cosine similarity with top N items\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    # A is the sparse matrix (user-item matrix)\n",
    "    # ntop is the number of top similar items you want\n",
    "    # lower_bound is the minimum similarity score to consider\n",
    "\n",
    "    # # Compute the top N cosine similarities in a sparse format\n",
    "    \n",
    "    C = sp_matmul_topn(A.T, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the user_business DataFrame to avoid issues with slicing\n",
    "user_business = user_business.copy()\n",
    "\n",
    "# Create user and business index mappings\n",
    "user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix (csr_matrix)\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace any NaN values with 0 in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)\n",
    "\n",
    "# Compute sparse cosine similarity matrix with top 10 most similar items\n",
    "item_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01,)\n",
    "\n",
    "# Convert sparse similarity matrix to a DataFrame (optional, for viewing)\n",
    "item_sim_sparse_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    item_similarity_sparse, \n",
    "    index=business_mapping.keys(), \n",
    "    columns=business_mapping.keys()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x24409d71140>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to SQLite (this will create a file-based database)\n",
    "conn = sqlite3.connect('../data/processed_data/yelp_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for storing user-item interactions and item-item similarities\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS ItemCF_user_item (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS ItemCF_item_item (\n",
    "    business_id TEXT,\n",
    "    similar_business_id TEXT,\n",
    "    similarity_score REAL\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the entire user-item interaction vectors (as JSON strings) with progress bar\n",
    "user_item_vectors = {user_idx: user_item_sparse[user_idx].toarray().tolist() for user_idx in range(user_item_sparse.shape[0])}\n",
    "print(\"Inserting user-item vectors:\")\n",
    "for user_id, vector in tqdm(user_mapping.items(), total=len(user_mapping), desc=\"User-Item Vectors\"):\n",
    "    cursor.execute('''\n",
    "        INSERT INTO ItemCF_user_item_vector (user_id, interaction_vector)\n",
    "        VALUES (?, ?)\n",
    "    ''', (user_id, json.dumps(user_item_vectors[vector])))\n",
    "\n",
    "# Insert the entire item-item similarity vectors (as JSON strings) with progress bar\n",
    "item_item_vectors = {biz_idx: item_similarity_sparse[biz_idx].toarray().tolist() for biz_idx in range(item_similarity_sparse.shape[0])}\n",
    "print(\"Inserting item-item vectors:\")\n",
    "for business_id, vector in tqdm(business_mapping.items(), total=len(business_mapping), desc=\"Item-Item Vectors\"):\n",
    "    cursor.execute('''\n",
    "        INSERT INTO ItemCF_item_item_vector (business_id, similarity_vector)\n",
    "        VALUES (?, ?)\n",
    "    ''', (business_id, json.dumps(item_item_vectors[vector])))\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
