{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Retrieval stage, we try to use the following methods/models to retrieve the relevant documents:\n",
    "1. Item-based Collaborative Filtering\n",
    "   - We only consider the `star` information that users give to the business in the `yelp_academic_dataset_review.json` file.\n",
    "   - We use `csr_matrix` from `scipy` to store the user-item matrix (for sparse matrix).\n",
    "   - We use `sp_matmul_topn` from `sparse_dot_topn` to calculate the cosine similarity to between two businesses. \n",
    "     - Compare to the `cosine_similarity` from `sklearn`, `sp_matmul_topn` is much faster as it only calculates the top-n similar items.\n",
    "     - Compare to the `approximate_nearest_neighbors` from `annoy`, `sp_matmul_topn` is much faster when finding building the index, but slower when querying. However, Item-based Collaborative Filtering \n",
    "2. User-based Collaborative Filtering\n",
    "3. Deep Structured Semantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_lists, prefix_path, chunk_size=10000):\n",
    "    df_dict = {}\n",
    "    prefix_path += \"sampled_\"\n",
    "    for file in file_lists:\n",
    "        try:\n",
    "            df_chunks = []\n",
    "            total_records = 0\n",
    "\n",
    "            for chunk in pd.read_json(prefix_path + file, lines=True, chunksize=chunk_size):\n",
    "                df_chunks.append(chunk)\n",
    "                total_records += chunk.shape[0]\n",
    "\n",
    "            df = pd.concat(df_chunks, ignore_index=True)\n",
    "            df_dict[file] = df\n",
    "            print(f\"Total records in {file}: {df.shape[0]}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../data/'\n",
    "transit_bucket = 'raw_datasets/'\n",
    "target_bucket = 'yelp/'\n",
    "prefix_path = folder_path + transit_bucket + target_bucket\n",
    "file_list = [\n",
    "    \"yelp_academic_dataset_business.json\",\n",
    "    \"yelp_academic_dataset_review.json\",\n",
    "    # \"yelp_academic_dataset_tip.json\",\n",
    "    # \"yelp_academic_dataset_checkin.json\",\n",
    "    # \"yelp_academic_dataset_user.json\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in yelp_academic_dataset_business.json: 78059.\n",
      "Total records in yelp_academic_dataset_review.json: 980418.\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset(file_list, prefix_path)\n",
    "df_business = df[\"yelp_academic_dataset_business.json\"]\n",
    "df_review = df[\"yelp_academic_dataset_review.json\"]\n",
    "\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sparse cosine similarity with top N items\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    # A is the sparse matrix (user-item matrix)\n",
    "    # ntop is the number of top similar items you want\n",
    "    # lower_bound is the minimum similarity score to consider\n",
    "\n",
    "    # # Compute the top N cosine similarities in a sparse format\n",
    "    \n",
    "    C = sp_matmul_topn(A.T, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the user_business DataFrame to avoid issues with slicing\n",
    "user_business = user_business.copy()\n",
    "\n",
    "# Create user and business index mappings\n",
    "user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix (csr_matrix)\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace any NaN values with 0 in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)\n",
    "\n",
    "# Compute sparse cosine similarity matrix with top 10 most similar items\n",
    "item_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01,)\n",
    "\n",
    "# Convert sparse similarity matrix to a DataFrame (optional, for viewing)\n",
    "# item_sim_sparse_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "#     item_similarity_sparse, \n",
    "#     index=business_mapping.keys(), \n",
    "#     columns=business_mapping.keys()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite (this will create a file-based database)\n",
    "db_path = '../data/processed_data/yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for user-item and item-item indexes\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS item_item_similarity (\n",
    "    item_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert user-item interaction matrix into SQLite\n",
    "def insert_user_item(user_business, conn, batch_size=10000):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert user-item interactions in batches\n",
    "    total_records = len(user_business)\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = user_business.iloc[i:i + batch_size]\n",
    "        \n",
    "        cursor.executemany('''INSERT INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''',\n",
    "                           batch[['user_id', 'business_id', 'stars_review']].values.tolist())\n",
    "        \n",
    "        conn.commit()  # Commit the batch\n",
    "        \n",
    "        # Show progress\n",
    "        print(f\"{i + len(batch)} / {total_records} records stored in user_item_index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert item vectors into SQLite\n",
    "def insert_item_vectors(item_similarity_sparse, business_mapping, conn, batch_size=1000, progress_interval=100000):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "\n",
    "    # Iterate over each row (item) in the sparse matrix\n",
    "    for row_idx in range(item_similarity_sparse.shape[0]):\n",
    "        # Get the row as a sparse vector (csr_matrix row)\n",
    "        row_vector = item_similarity_sparse.getrow(row_idx)\n",
    "\n",
    "        # Serialize the row vector using pickle\n",
    "        serialized_row = pickle.dumps(row_vector)\n",
    "\n",
    "        # Get the item id (business_id)\n",
    "        item_id = list(business_mapping.keys())[row_idx]\n",
    "\n",
    "        # Add the item and its vector to the batch\n",
    "        batch.append((item_id, serialized_row))\n",
    "\n",
    "        # Insert in batches to reduce the number of commits\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            conn.commit()  # Commit the batch\n",
    "            total_inserted += len(batch)\n",
    "\n",
    "            # Print progress every 100,000 records\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} item vectors so far...\")\n",
    "\n",
    "            batch = []  # Clear the batch after committing\n",
    "\n",
    "    # Insert any remaining records after the loop\n",
    "    if batch:\n",
    "        cursor.executemany('''INSERT INTO item_item_similarity (item_id, similarity_vector)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        conn.commit()\n",
    "        total_inserted += len(batch)\n",
    "\n",
    "    # Final progress message\n",
    "    print(f\"Total {total_inserted} item vectors inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 / 985732 records stored in user_item_index\n",
      "20000 / 985732 records stored in user_item_index\n",
      "30000 / 985732 records stored in user_item_index\n",
      "40000 / 985732 records stored in user_item_index\n",
      "50000 / 985732 records stored in user_item_index\n",
      "60000 / 985732 records stored in user_item_index\n",
      "70000 / 985732 records stored in user_item_index\n",
      "80000 / 985732 records stored in user_item_index\n",
      "90000 / 985732 records stored in user_item_index\n",
      "100000 / 985732 records stored in user_item_index\n",
      "110000 / 985732 records stored in user_item_index\n",
      "120000 / 985732 records stored in user_item_index\n",
      "130000 / 985732 records stored in user_item_index\n",
      "140000 / 985732 records stored in user_item_index\n",
      "150000 / 985732 records stored in user_item_index\n",
      "160000 / 985732 records stored in user_item_index\n",
      "170000 / 985732 records stored in user_item_index\n",
      "180000 / 985732 records stored in user_item_index\n",
      "190000 / 985732 records stored in user_item_index\n",
      "200000 / 985732 records stored in user_item_index\n",
      "210000 / 985732 records stored in user_item_index\n",
      "220000 / 985732 records stored in user_item_index\n",
      "230000 / 985732 records stored in user_item_index\n",
      "240000 / 985732 records stored in user_item_index\n",
      "250000 / 985732 records stored in user_item_index\n",
      "260000 / 985732 records stored in user_item_index\n",
      "270000 / 985732 records stored in user_item_index\n",
      "280000 / 985732 records stored in user_item_index\n",
      "290000 / 985732 records stored in user_item_index\n",
      "300000 / 985732 records stored in user_item_index\n",
      "310000 / 985732 records stored in user_item_index\n",
      "320000 / 985732 records stored in user_item_index\n",
      "330000 / 985732 records stored in user_item_index\n",
      "340000 / 985732 records stored in user_item_index\n",
      "350000 / 985732 records stored in user_item_index\n",
      "360000 / 985732 records stored in user_item_index\n",
      "370000 / 985732 records stored in user_item_index\n",
      "380000 / 985732 records stored in user_item_index\n",
      "390000 / 985732 records stored in user_item_index\n",
      "400000 / 985732 records stored in user_item_index\n",
      "410000 / 985732 records stored in user_item_index\n",
      "420000 / 985732 records stored in user_item_index\n",
      "430000 / 985732 records stored in user_item_index\n",
      "440000 / 985732 records stored in user_item_index\n",
      "450000 / 985732 records stored in user_item_index\n",
      "460000 / 985732 records stored in user_item_index\n",
      "470000 / 985732 records stored in user_item_index\n",
      "480000 / 985732 records stored in user_item_index\n",
      "490000 / 985732 records stored in user_item_index\n",
      "500000 / 985732 records stored in user_item_index\n",
      "510000 / 985732 records stored in user_item_index\n",
      "520000 / 985732 records stored in user_item_index\n",
      "530000 / 985732 records stored in user_item_index\n",
      "540000 / 985732 records stored in user_item_index\n",
      "550000 / 985732 records stored in user_item_index\n",
      "560000 / 985732 records stored in user_item_index\n",
      "570000 / 985732 records stored in user_item_index\n",
      "580000 / 985732 records stored in user_item_index\n",
      "590000 / 985732 records stored in user_item_index\n",
      "600000 / 985732 records stored in user_item_index\n",
      "610000 / 985732 records stored in user_item_index\n",
      "620000 / 985732 records stored in user_item_index\n",
      "630000 / 985732 records stored in user_item_index\n",
      "640000 / 985732 records stored in user_item_index\n",
      "650000 / 985732 records stored in user_item_index\n",
      "660000 / 985732 records stored in user_item_index\n",
      "670000 / 985732 records stored in user_item_index\n",
      "680000 / 985732 records stored in user_item_index\n",
      "690000 / 985732 records stored in user_item_index\n",
      "700000 / 985732 records stored in user_item_index\n",
      "710000 / 985732 records stored in user_item_index\n",
      "720000 / 985732 records stored in user_item_index\n",
      "730000 / 985732 records stored in user_item_index\n",
      "740000 / 985732 records stored in user_item_index\n",
      "750000 / 985732 records stored in user_item_index\n",
      "760000 / 985732 records stored in user_item_index\n",
      "770000 / 985732 records stored in user_item_index\n",
      "780000 / 985732 records stored in user_item_index\n",
      "790000 / 985732 records stored in user_item_index\n",
      "800000 / 985732 records stored in user_item_index\n",
      "810000 / 985732 records stored in user_item_index\n",
      "820000 / 985732 records stored in user_item_index\n",
      "830000 / 985732 records stored in user_item_index\n",
      "840000 / 985732 records stored in user_item_index\n",
      "850000 / 985732 records stored in user_item_index\n",
      "860000 / 985732 records stored in user_item_index\n",
      "870000 / 985732 records stored in user_item_index\n",
      "880000 / 985732 records stored in user_item_index\n",
      "890000 / 985732 records stored in user_item_index\n",
      "900000 / 985732 records stored in user_item_index\n",
      "910000 / 985732 records stored in user_item_index\n",
      "920000 / 985732 records stored in user_item_index\n",
      "930000 / 985732 records stored in user_item_index\n",
      "940000 / 985732 records stored in user_item_index\n",
      "950000 / 985732 records stored in user_item_index\n",
      "960000 / 985732 records stored in user_item_index\n",
      "970000 / 985732 records stored in user_item_index\n",
      "980000 / 985732 records stored in user_item_index\n",
      "985732 / 985732 records stored in user_item_index\n",
      "Total 78059 item vectors inserted.\n"
     ]
    }
   ],
   "source": [
    "# Insert user-item index with progress\n",
    "insert_user_item(user_business, conn)\n",
    "\n",
    "# Insert item vectors into the database\n",
    "insert_item_vectors(item_similarity_sparse, business_mapping, conn)\n",
    "\n",
    "\n",
    "# Close the connection when done\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Retrieve the first one hundred item vectors\n",
    "cursor.execute('SELECT similarity_vector FROM item_item_similarity LIMIT 100')\n",
    "serialized_vectors = cursor.fetchall()\n",
    "\n",
    "# Deserialize all vectors\n",
    "item_vectors = [pickle.loads(row[0]) for row in serialized_vectors]\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "       25., 25., 25., 25., 25., 25., 25., 20., 20., 10.,  5.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the stored elements in the first item vector\n",
    "item_vectors[0].data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
